{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8a0f72-5f81-4ce3-ab23-0fd1a6da8c78",
   "metadata": {},
   "source": [
    "Logistic regression on the epsilon dataset\n",
    "==========================================\n",
    "\n",
    "_Chuan-Zheng Lee <<czlee@stanford.edu>>_ <br />\n",
    "_July 2021_\n",
    "\n",
    "This is a \"getting started\" exercise. Simple logistic regression on the [epsilon dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#epsilon), which contains 400,000 training data points with 2,000 features, and 100,000 test data points.\n",
    "\n",
    "This notebook is mostly to try things out. The \"real\" script is in ../logistic.py. To run this locally, I used a smaller version of the epsilon dataset, constructed by taking the first 1000 lines of the test set as the \"smaller training set\", and the last 200 lines of the test set as the \"smaller test set\", as follows (in bash, replace `~/jadeite/data/sources` with wherever your data directory is):\n",
    "\n",
    "``` bash\n",
    "mkdir -p ~/jadeite/data/sources/epsilon\n",
    "cd ~/jadeite/data/sources/epsilon\n",
    "wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/epsilon_normalized.t.bz2\n",
    "bunzip2 epsilon_normalized.t.bz2\n",
    "mv epsilon_normalized.t epsilon_normalized.t.full\n",
    "head epsilon_normalized.t.full -n 1000 > epsilon_normalized\n",
    "tail epsilon_normalized.t.full -n 200 > epsilon_normalized.t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6780e9-dca9-4542-a201-680a229eb919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:42.433108: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-04 17:38:42.433159: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import data.epsilon_tf as epsilon\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08d73ef-ef3d-474b-a086-2f3a746fadec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 2,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:43.382481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-04 17:38:43.382644: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-04 17:38:43.382676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (zinfandel): /proc/driver/nvidia/version does not exist\n",
      "2021-07-04 17:38:43.382945: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a87a69-e344-42ff-b4c6-e2ba4eb53ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/czlee/jadeite/data/sources/epsilon/epsilon_normalized, up to line 800... done.\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5115\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5135\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5229\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5219\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:44.382453: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-04 17:38:44.383664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1992005000 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa510deda30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer='sgd', metrics=['accuracy'])\n",
    "nepochs = 5\n",
    "batch_size = 64\n",
    "dataset = epsilon.train_dataset().repeat(nepochs).batch(batch_size)\n",
    "model.fit(dataset, epochs=nepochs, steps_per_epoch = epsilon.ntrain // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22185e90-0057-47c0-bb06-fe2a7a788d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 49ms/step - loss: 0.6956 - accuracy: 0.4350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6956179141998291, 'accuracy': 0.4350000023841858}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = epsilon.test_dataset().batch(batch_size)\n",
    "evaluation = model.evaluate(test_dataset, return_dict=True)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494054c9-9625-4888-8852-dc48d8cc54e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 5, 0 of 3, loss: 0.695436\r"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "])\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 5\n",
    "batch_size = 64\n",
    "nbatches = epsilon.ntest // batch_size\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    dataset = epsilon.test_dataset().batch(batch_size)\n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        with tf.GradientTape() as tape:\n",
    "            ŷ = model(x)\n",
    "            loss = loss_fn(y, ŷ)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch {epoch} of {nepochs}, {i} of {nbatches}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bfbace-5f2e-486d-b309-9eade56bb7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 0...\n",
      "Accuracy: 0.5350000262260437\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "dataset = epsilon.test_dataset().batch(batch_size)\n",
    "nbatches = epsilon.ntest // batch_size\n",
    "accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "for i, (x, y) in dataset.enumerate():\n",
    "    ŷ = model(x)\n",
    "    accuracy_fn.update_state(y, ŷ)\n",
    "    print(f\"{i} of {nbatches}...\", end='\\r')\n",
    "accuracy = accuracy_fn.result().numpy()\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26f083-60b2-40e2-8ba7-952c936c6fa3",
   "metadata": {},
   "source": [
    "# Simple federated averaging\n",
    "\n",
    "Again, mostly an exercise, this is an attempt to use the tensorflow-federated framework with federated averaging to achieve the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5be377-9518-49e2-a627-13a05219aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbac75c-085b-4574-8180-40be030e9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4fe6e-286c-4e2b-bb89-50d991c35593",
   "metadata": {},
   "source": [
    "The `Dataset.shard()` method divides a dataset into several shards. Originally I had something like this:\n",
    "\n",
    "``` python\n",
    "def client_data_by_shard(client_id):\n",
    "    return train_dataset.shard(nclients, client_id)\n",
    "\n",
    "client_data = tff.simulation.datasets.ClientData.from_clients_and_fn(range(nclients), client_data_by_shard)\n",
    "```\n",
    "\n",
    "but we don't actually need a `ClientData` object, since TFF just takes in lists of `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff3e0de-989a-4cc7-a5f0-1cabef694bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/czlee/jadeite/data/sources/epsilon/epsilon_normalized, up to line 800... done.\n"
     ]
    }
   ],
   "source": [
    "nclients = 10\n",
    "nrounds = 8\n",
    "batch_size = 64\n",
    "train_dataset = epsilon.train_dataset().batch(batch_size)\n",
    "client_shards = [train_dataset.shard(nclients, i) for i in range(nclients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44818506-6390-4d33-ab59-85877cb6936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 2000), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34780ac2-f4dc-478d-b698-615e250b2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "    ])\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=train_dataset.element_spec,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42738c5c-c5fc-440a-a735-5cb1b8900a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: /home/czlee/jadeite/results/20210704-173849\n",
      "WARNING:tensorflow:From /home/czlee/jadeite/venv/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/czlee/jadeite/venv/lib/python3.8/site-packages/tensorflow_federated/python/core/impl/compiler/tensorflow_computation_transformations.py:59: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ServerState(model=ModelWeights(trainable=[array([[-0.03082578],\n",
       "       [-0.04259921],\n",
       "       [-0.01149698],\n",
       "       ...,\n",
       "       [ 0.03364855],\n",
       "       [-0.00396271],\n",
       "       [-0.04661773]], dtype=float32), array([0.], dtype=float32)], non_trainable=[]), optimizer_state=[0], delta_aggregate_state=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), model_broadcast_state=())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir = utils.create_results_directory()\n",
    "log_dir = results_dir / 'logs'\n",
    "summary_writer = tf.summary.create_file_writer(str(log_dir))  # doesn't support Path objects\n",
    "\n",
    "state = iterative_process.initialize()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04e1c0f-3934-459f-8609-48c8bac239a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0 of 8...\n",
      "round 1 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:55.108429: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:55.771739: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 3 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:56.576080: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 4 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:57.240788: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 5 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:57.878441: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 6 of 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 17:38:58.508447: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 7 of 8...\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    for r in range(nrounds):\n",
    "        print(f\"round {r} of {nrounds}...\")\n",
    "        state, metrics = iterative_process.next(state, client_shards)\n",
    "        for name, value in metrics['train'].items():\n",
    "            tf.summary.scalar(name, value, step=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d72379-c798-44f1-b624-4b0418531579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('binary_accuracy', 0.48710936), ('loss', 0.69323754)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca124d52-11f1-4723-884a-831ede821144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b204058b6d963bca\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b204058b6d963bca\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38443c4-b087-4327-8468-c7f3eb50bd60",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e3a04db-ed9c-4885-8c70-7321d66d56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 67ms/step - loss: 0.6938 - binary_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.693757176399231, 0.5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = create_keras_model()\n",
    "test_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "state.model.assign_weights_to(test_model)\n",
    "test_dataset = epsilon.test_dataset().batch(batch_size)\n",
    "test_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a490687-74cf-4b6c-953c-d9c91ffbd285",
   "metadata": {},
   "source": [
    "# Federated averaging done \"manually\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940f7bbe-b797-4e93-a4d4-c8c4eafa253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrounds = 5\n",
    "nclients = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "    ])\n",
    "\n",
    "def client_train(dataset, model, loss_fn, optimizer):\n",
    "    for x, y in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(y, pred)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    return loss.numpy()\n",
    "\n",
    "\n",
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"Aggregates client models by just taking the mean, a.k.a.\n",
    "    federated averaging.\"\"\"\n",
    "    client_weights = [model.get_weights() for model in client_models]\n",
    "    new_weights = [\n",
    "        tf.math.reduce_mean(tf.stack(weights, axis=0), axis=0)\n",
    "        for weights in zip(*client_weights)\n",
    "    ]\n",
    "    for model in client_models:\n",
    "        model.set_weights(new_weights)\n",
    "\n",
    "\n",
    "def test(dataset, model, loss_fn, accuracy_fn):\n",
    "    test_losses = []\n",
    "    accuracy_fn.reset_state()\n",
    "    for x, y in dataset:\n",
    "        pred = model(x)\n",
    "        accuracy_fn.update_state(y, pred)\n",
    "        test_losses.append(loss_fn(y, pred))\n",
    "\n",
    "    test_loss = tf.math.reduce_mean(test_losses).numpy()\n",
    "    accuracy = accuracy_fn.result().numpy()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54bcb042-434e-4c70-8abd-d2f86734ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 2,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "global_model = create_model()\n",
    "client_models = [create_model() for i in range(nclients)]\n",
    "for model in client_models:\n",
    "    model.set_weights(global_model.get_weights())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95786a6-3c4a-453a-b3c3-afb631fda908",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "client_optimizers = [tf.keras.optimizers.SGD(learning_rate=1e-2) for i in range(nclients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071ab13f-73ad-47f1-95f0-fc0587083746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 of 10: loss 0.6925334930419922\n",
      "Client 1 of 10: loss 0.6869416832923889\n",
      "Client 2 of 10: loss 0.6913622617721558\n",
      "Client 3 of 10: loss 0.692907452583313\n",
      "Client 4 of 10: loss 0.6913371086120605\n",
      "Client 5 of 10: loss 0.6915407180786133\n",
      "Client 6 of 10: loss 0.6922076940536499\n",
      "Client 7 of 10: loss 0.6917317509651184\n",
      "Client 8 of 10: loss 0.6906509399414062\n",
      "Client 9 of 10: loss 0.6940387487411499\n"
     ]
    }
   ],
   "source": [
    "# train clients, just one epoch\n",
    "clients = zip(client_shards, client_models, client_optimizers)\n",
    "for i, (dataset, model, optimizer) in enumerate(clients):\n",
    "    loss = client_train(dataset, model, loss_fn, optimizer)\n",
    "    print(f\"Client {i} of {nclients}: loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5172dfa9-c9e8-4a9f-af25-e12b353623f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.05350032],\n",
       "        [-0.05351546],\n",
       "        [ 0.04530976],\n",
       "        ...,\n",
       "        [-0.04561194],\n",
       "        [ 0.02014246],\n",
       "        [ 0.04896324]], dtype=float32),\n",
       " array([-0.00151702], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_models[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d2b347a-4ed8-40b2-9a7d-39f838c23e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.05349647],\n",
       "        [-0.05352294],\n",
       "        [ 0.04531534],\n",
       "        ...,\n",
       "        [-0.04560898],\n",
       "        [ 0.02012952],\n",
       "        [ 0.04897283]], dtype=float32),\n",
       " array([-0.00135763], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_models[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1195a5b8-19ec-4950-92c5-3f4582c34e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(10, 2000, 1), dtype=float32, numpy=\n",
       " array([[[ 0.05350032],\n",
       "         [-0.05351546],\n",
       "         [ 0.04530976],\n",
       "         ...,\n",
       "         [-0.04561194],\n",
       "         [ 0.02014246],\n",
       "         [ 0.04896324]],\n",
       " \n",
       "        [[ 0.05349647],\n",
       "         [-0.05352294],\n",
       "         [ 0.04531534],\n",
       "         ...,\n",
       "         [-0.04560898],\n",
       "         [ 0.02012952],\n",
       "         [ 0.04897283]],\n",
       " \n",
       "        [[ 0.05347595],\n",
       "         [-0.05350532],\n",
       "         [ 0.04529609],\n",
       "         ...,\n",
       "         [-0.04559762],\n",
       "         [ 0.02013243],\n",
       "         [ 0.04894402]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.05348852],\n",
       "         [-0.05352173],\n",
       "         [ 0.04530226],\n",
       "         ...,\n",
       "         [-0.04562178],\n",
       "         [ 0.02014896],\n",
       "         [ 0.04893569]],\n",
       " \n",
       "        [[ 0.05350902],\n",
       "         [-0.05348571],\n",
       "         [ 0.04531093],\n",
       "         ...,\n",
       "         [-0.0456634 ],\n",
       "         [ 0.02016379],\n",
       "         [ 0.04889783]],\n",
       " \n",
       "        [[ 0.05346051],\n",
       "         [-0.05350388],\n",
       "         [ 0.04530965],\n",
       "         ...,\n",
       "         [-0.04565586],\n",
       "         [ 0.02011865],\n",
       "         [ 0.04889026]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[-0.00151702],\n",
       "        [-0.00135763],\n",
       "        [-0.00089902],\n",
       "        [ 0.00145612],\n",
       "        [ 0.00127499],\n",
       "        [ 0.00020991],\n",
       "        [-0.00108565],\n",
       "        [ 0.00049551],\n",
       "        [-0.00108455],\n",
       "        [ 0.00017355]], dtype=float32)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_weights = [model.get_weights() for model in client_models]\n",
    "[tf.stack(weights, axis=0) for weights in zip(*client_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6e8e93-f753-46c8-8684-5167f6b88bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_aggregate(global_model, client_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a81684b-7dc1-49c6-85c0-1c013fa22628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.05347797],\n",
       "        [-0.05350733],\n",
       "        [ 0.04529819],\n",
       "        ...,\n",
       "        [-0.0456288 ],\n",
       "        [ 0.02014136],\n",
       "        [ 0.04891669]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81278842-049a-4005-bd8f-cbc27e82458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.6929768323898315, accuracy 0.51953125\n"
     ]
    }
   ],
   "source": [
    "test_loss, accuracy = test(test_dataset, global_model, loss_fn, accuracy_fn)\n",
    "print(f\"test loss {test_loss}, accuracy {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
