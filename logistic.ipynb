{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8a0f72-5f81-4ce3-ab23-0fd1a6da8c78",
   "metadata": {},
   "source": [
    "# Logistic regression on the epsilon dataset\n",
    "\n",
    "This is a \"getting started\" exercise. Simple logistic regression on the [epsilon dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#epsilon), which contains 400,000 training data points with 2,000 features, and 100,000 test data points.\n",
    "\n",
    "The training data file is about 12 GB uncompressed, so this uses the test data (about 3 GB) to get things going. When doing this for real we would obviously use the training data for training, not the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6780e9-dca9-4542-a201-680a229eb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08d73ef-ef3d-474b-a086-2f3a746fadec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 2,001\n",
      "Trainable params: 2,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a87a69-e344-42ff-b4c6-e2ba4eb53ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1562/1562 [==============================] - 117s 75ms/step - loss: 0.6919 - accuracy: 0.5402\n",
      "Epoch 2/5\n",
      "1562/1562 [==============================] - 105s 67ms/step - loss: 0.6886 - accuracy: 0.6222\n",
      "Epoch 3/5\n",
      "1562/1562 [==============================] - 104s 67ms/step - loss: 0.6854 - accuracy: 0.6632\n",
      "Epoch 4/5\n",
      "1562/1562 [==============================] - 121s 77ms/step - loss: 0.6823 - accuracy: 0.6856\n",
      "Epoch 5/5\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.6973 ETAWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 7812.5 batches). You may need to use the repeat() function when building your dataset.\n",
      "1562/1562 [==============================] - 135s 86ms/step - loss: 0.6793 - accuracy: 0.6973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f59c41158b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer='sgd', metrics=['accuracy'])\n",
    "nepochs = 5\n",
    "batch_size = 64\n",
    "dataset = epsilon.test_dataset().repeat(nepochs).batch(batch_size)\n",
    "model.fit(dataset, epochs=nepochs, steps_per_epoch = epsilon.ntest / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494054c9-9625-4888-8852-dc48d8cc54e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 of 5, 1560 of 1562, loss: 0.677202\r"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2000,)),\n",
    "])\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 5\n",
    "batch_size = 64\n",
    "nbatches = epsilon.ntest // batch_size\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    dataset = epsilon.test_dataset().batch(batch_size)\n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        with tf.GradientTape() as tape:\n",
    "            ŷ = model(x)\n",
    "            loss = loss_fn(y, ŷ)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch {epoch} of {nepochs}, {i} of {nbatches}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bfbace-5f2e-486d-b309-9eade56bb7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 of 100...\n",
      "Accuracy: 0.6976001262664795\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "dataset = epsilon.test_dataset().batch(batch_size)\n",
    "nbatches = epsilon.ntest // batch_size\n",
    "accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "for i, (x, y) in dataset.enumerate():\n",
    "    ŷ = model(x)\n",
    "    accuracy_fn.update_state(y, ŷ)\n",
    "    print(f\"{i} of {nbatches}...\", end='\\r')\n",
    "accuracy = accuracy_fn.result().numpy()\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
