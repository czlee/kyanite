{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58ff68b-53d0-4cae-8f6f-37b168c8b475",
   "metadata": {},
   "source": [
    "First proof of concept\n",
    "======================\n",
    "\n",
    "_Chuan-Zheng Lee <<czlee@stanford.edu>>_ <br />\n",
    "_July 2021_\n",
    "\n",
    "Here's the idea in this notebook:\n",
    "\n",
    "- We take the most basic nontrivial neural network task we can think ofâ€”I nominate the [MNIST digit recognition task](https://keras.io/examples/vision/mnist_convnet/).\n",
    "- Run a well-known, impossible-to-fail training system on that\n",
    "- Code up the new [`Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer) object, and try that out\n",
    "- Does it work? How does it compare?\n",
    "\n",
    "This will not be journal-ready, but it will provide a short development cycle for our new `Optimizer`.\n",
    "\n",
    "If we get bored of this task, there are plenty more basic working examples in https://keras.io/examples/vision/. Again, not saying we should use these in our paper, but they'll get us started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c31fe1-0f53-433b-acb6-e5252ed8a078",
   "metadata": {},
   "source": [
    "Really basic MNIST task\n",
    "-----------------------\n",
    "\n",
    "This code is literally lifted straight out of https://keras.io/examples/vision/mnist_convnet/, except that I changed the optimizer to SGD (it's Adam in the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b1c443-0540-4087-ac0b-1b387f233c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5345e96e-7eed-456e-987e-c0d561b1dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f567237-4025-497b-a50c-46bd143f3e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fbb746-b4a3-48ce-b7fc-912fa9f2a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 1.6412 - accuracy: 0.4810 - val_loss: 0.5338 - val_accuracy: 0.8793\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.5821 - accuracy: 0.8178 - val_loss: 0.2735 - val_accuracy: 0.9320\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 0.3793 - accuracy: 0.8849 - val_loss: 0.1994 - val_accuracy: 0.9468\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.3035 - accuracy: 0.9091 - val_loss: 0.1664 - val_accuracy: 0.9568\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.2617 - accuracy: 0.9219 - val_loss: 0.1462 - val_accuracy: 0.9615\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.2304 - accuracy: 0.9301 - val_loss: 0.1319 - val_accuracy: 0.9657\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 22s 51ms/step - loss: 0.2086 - accuracy: 0.9379 - val_loss: 0.1219 - val_accuracy: 0.9677\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 25s 59ms/step - loss: 0.1951 - accuracy: 0.9410 - val_loss: 0.1125 - val_accuracy: 0.9693\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 27s 64ms/step - loss: 0.1824 - accuracy: 0.9450 - val_loss: 0.1060 - val_accuracy: 0.9703\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.1703 - accuracy: 0.9480 - val_loss: 0.1000 - val_accuracy: 0.9718\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 18s 44ms/step - loss: 0.1630 - accuracy: 0.9519 - val_loss: 0.0956 - val_accuracy: 0.9735\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.1541 - accuracy: 0.9530 - val_loss: 0.0923 - val_accuracy: 0.9735\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.1504 - accuracy: 0.9546 - val_loss: 0.0881 - val_accuracy: 0.9757\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 22s 51ms/step - loss: 0.1428 - accuracy: 0.9570 - val_loss: 0.0853 - val_accuracy: 0.9757\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 27s 64ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.0823 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b74dd5970>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "nepochs = 15\n",
    "\n",
    "# I changed the optimizer to SGD (it was Adam), and instantiated an Optimizer object to make\n",
    "# it clearer when we write our own optimizer.\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nepochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab53f7a-66bc-4cf0-bb7a-c9ebc3e6b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0861344262957573\n",
      "Test accuracy: 0.9742000102996826\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b77981-87ee-41b6-80ed-7f3cccb4c606",
   "metadata": {},
   "source": [
    "# Custom optimizer\n",
    "\n",
    "Nah, bad idea.\n",
    "\n",
    "# Opening the gradient update loop\n",
    "\n",
    "This code is sort of taken from https://keras.io/getting_started/intro_to_keras_for_researchers/#layer-gradients, though it is adapted. Differences from the code in that tutorial (which uses the same dataset, but a different model), mostly to be consistent with the basic MNIST tutorial we used above:\n",
    "- We use the more complicated network architecture that we used above, not the simple three-layer network in the research tutorial\n",
    "- The output is a probability vector (softmax), not logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f34911-5e39-4de9-b592-3ae7959e751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 of 15, 900 of 937, loss: 0.117984\r"
     ]
    }
   ],
   "source": [
    "# running eagerly\n",
    "\n",
    "# same model, loss function and optimizer as before, but instantiate new ones\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 15\n",
    "batch_size = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "n = x_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    \n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = model(x)            # forward pass\n",
    "            loss = loss_fn(y, probs)    # external loss value\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_weights)            # compute gradients\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))  # apply gradients\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"epoch {epoch} of {nepochs}, {i} of {n}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0959f9be-dd75-4566-a86d-bf5dce40fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0636865571141243\n",
      "Test accuracy: 0.9805999994277954\n"
     ]
    }
   ],
   "source": [
    "probs_test = model(x_test)\n",
    "test_loss = loss_fn(y_test, probs_test)\n",
    "accuracy_fn = keras.metrics.CategoricalAccuracy()\n",
    "test_accuracy = accuracy_fn(probs_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", float(test_loss))\n",
    "print(\"Test accuracy:\", float(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2350c6d8-dfc4-4b9a-8c87-456ee4d91ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 of 15, 900 of 937, loss: 0.152469\r"
     ]
    }
   ],
   "source": [
    "# running with compiled function\n",
    "\n",
    "# same model, loss function and optimizer as before, but instantiate new ones\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 15\n",
    "batch_size = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "n = x_train.shape[0] // batch_size\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(x)\n",
    "        loss = loss_fn(y, probs)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        loss = train_on_batch(x, y)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"epoch {epoch} of {nepochs}, {i} of {n}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85d0116-dde6-4cb3-9959-ae6f1e19ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.06301199644804001\n",
      "Test accuracy: 0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "probs_test = model(x_test)\n",
    "test_loss = loss_fn(y_test, probs_test)\n",
    "accuracy_fn = keras.metrics.CategoricalAccuracy()\n",
    "test_accuracy = accuracy_fn(probs_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", float(test_loss))\n",
    "print(\"Test accuracy:\", float(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a9f13-b398-4152-84c7-b36aefe54c78",
   "metadata": {},
   "source": [
    "# Larger batch sizes\n",
    "\n",
    "In some sense, our algorithm is a variant on full gradient descent, so I guess we should be sure that we can do that. Thoughâ€¦ 60,000 is a lot. Maybe let's just try larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9a4cb6-91bc-4883-b658-5ef8f0b8934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 of 15, 119 of 120, loss: 0.228805\r"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 15\n",
    "batch_size = 500\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "n = x_train.shape[0] // batch_size\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(x)\n",
    "        loss = loss_fn(y, probs)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        loss = train_on_batch(x, y)\n",
    "        print(f\"epoch {epoch} of {nepochs}, {i} of {n}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40b3e09-c39a-4127-9529-1dd46cc04e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.19686318933963776\n",
      "Test accuracy: 0.9431999921798706\n"
     ]
    }
   ],
   "source": [
    "probs_test = model(x_test)\n",
    "test_loss = loss_fn(y_test, probs_test)\n",
    "accuracy_fn = keras.metrics.CategoricalAccuracy()\n",
    "test_accuracy = accuracy_fn(probs_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", float(test_loss))\n",
    "print(\"Test accuracy:\", float(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa32fa9-e345-4e80-a777-612eb1b5a0cb",
   "metadata": {},
   "source": [
    "# Adding random noise to the gradients\n",
    "\n",
    "Here's a really dumb idea that isn't quite what we mean: Just add some random noise to the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b90e1a0e-f574-466e-a6ef-15c7ec92b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 of 15, 119 of 120, loss: 0.775801\r"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "nepochs = 15\n",
    "batch_size = 500\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "n = x_train.shape[0] // batch_size\n",
    "Ïƒâ‚™ = 1.0\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(x)\n",
    "        loss = loss_fn(y, probs)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    gradients = [g + tf.random.normal(shape=g.shape, mean=0.0, stddev=Ïƒâ‚™) for g in gradients]\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    for i, (x, y) in dataset.enumerate():\n",
    "        loss = train_on_batch(x, y)\n",
    "        print(f\"epoch {epoch} of {nepochs}, {i} of {n}, loss: {loss:f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41da35af-2fa4-4a09-9eeb-5fbdff1dab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8540118932723999\n",
      "Test accuracy: 0.807200014591217\n"
     ]
    }
   ],
   "source": [
    "probs_test = model(x_test)\n",
    "test_loss = loss_fn(y_test, probs_test)\n",
    "accuracy_fn = keras.metrics.CategoricalAccuracy()\n",
    "test_accuracy = accuracy_fn(probs_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", float(test_loss))\n",
    "print(\"Test accuracy:\", float(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
